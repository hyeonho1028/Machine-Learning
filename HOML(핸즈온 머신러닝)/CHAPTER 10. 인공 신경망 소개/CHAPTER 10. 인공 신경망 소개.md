# CHAPTER 10. 인공 신경망 소개

> 지능적인 기계에 대한 영감을 얻기 위해서는 뇌의 구조를 살펴보는 것이 합리적이다. 이것이 ANN(artificial neural networks)을 만들어낸 핵심 아이디어이다.



- 인공 신경망은 딥러닝의 핵심이다. 인공 신경망은 다재다능하고 강력하고 확장성이 좋아서 수백만 개의 이미지를 분류하거나, 음성 인식 서비스의 성능을 높이거나, 매일 수억 명의 사용자에게 가장 좋은 비디오를 추천해주거나, 바둑 세계챔피언을 이기기 위해 수백만 개의 기보를 익히고 자기 자신과 게임하면서 학습하는 경우 등 아주 복잡한 대규모 머신러닝 문제를 다루는 데 적합하다.
- 이 장에서는 인공 신경망의 초창기 구조를 간단히 소개하는 것으로 시작하며 그 후 다층 퍼셉트론(Multi-Layer Perceptron)을 설명한다.



### 10.1 생물학적 뉴런에서 인공 뉴런까지

> 신경망은 꽤 오래전부터 존재했다. 1943년 신경생리학자 워런 맥컬록과 수학자 월터 피츠가 이를 처음 소개했다. 그러나 곧 한계점에 도달했고, 다른 분야로 투자가 옮겨지게 되었다. 1990년대까지 SVM같은 강력한 머신러닝 기법을 더 선호했다. 이론적 기반이 탄탄했기 때문이다. 그러나 인공 신경망의 부흥이 시작되고 있다. 예전과는 다르게 우리 생활에 훨씬 커다란 영향을 줄 것인가?
>
> 요즘엔 신경망을 훈련시키기 위한 데이터가 매우 많다. 인공 신경망은 종종 규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능을 낸다.



#### 10.1.1 생물학적 뉴런

인간의 뉴런은 세포체와 수상돌기, 축삭돌기를 가지고 있다. 이것은 축삭끝가지, 시냅스 말단 등의 구조로 나뉘고 뉴런끼리 짧은 전기 자극 신호를 주고 받는다. 개개의 생물학적 뉴런은 아주 단순하게 동작하지만, 수십억 개의 뉴런으로 구성된 거대한 네트워크로 조직되어 있고 각 뉴런은 보통 수천 개의 다른 뉴런과 연결되어 있다. 생물학적 뉴런은 아직까지도 활발히 연구가 진행되는 분야이며 뇌의 일부를 살펴보면 뉴런들이 연속된 층으로 조직화되어 있는 모습을 볼 수 있다.



#### 10.1.2 뉴런을 사용한 논리 연산

초창기의 뉴런은 매우 단순했다. 하나 이상의 이진(on/off) 입력과 하나의 이진 출력을 가졌다. 이 단순한 인공 뉴런은 일정 개수의 입력이 활성화되었을 때 출력을 내보냈다.

> 2개의 인공 뉴런이 존재하면 간단한 논리 연산인 항등함수, 논리곱 연산, 논리합 연산, 논리 부정 연산 등을 구현 할 수 있다.
>
> 맥컬록과 피츠의 이름을 따서 MCP 뉴런이라고도 한다.



### 10.1.3 퍼셉트론

Perceptron은 가장 간단한 인공 신경망 구조 중 하나로 1967년 프랑크 로젠블라트가 제안했다. 퍼셉트론은 TLU(threshold logic unit)라는 조금 다른 형태의 인공 뉴런을 기반으로 한다. 입력과 출력이 (on/off)가 아니라 어떤 숫자고 각각의 입력 연결은 가중치과 연관이 있다. TLU는 입력의 가중치 합을 계산하고 $(z=w_1x_1 + w_2x_2 + ... + w_nx_n = w^T  x)$, 그런 다음 계산된 합에 계단 함수를 적용하여 그 결과를 출력한다.

즉 $h_w(x)=step(z)=step(w^T x)​$라고 할 수 있다.

퍼셉트론에서 가장 널리 사용되는 계단 함수는 헤비사이트 계단 함수 이다. 이따금 부호 함수 sign function를 대신 사용하기도 한다.

**heaviside step function**

![H[n]=\begin{cases} 0, & n < 0, \\ 1, & n \ge 0, \end{cases} ](https://wikimedia.org/api/rest_v1/media/math/render/svg/f1783c84465f7a602fae566c34efa63f48c84212)

**sign function**

![\operatorname{sgn}(x):={\begin{cases}-1&{\text{if }}x<0,\\0&{\text{if }}x=0,\\1&{\text{if }}x>0.\end{cases}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f8374ef8411ba954fb2655992b6e4496b8710cd6)



하나의 TLU는 간단한 선형 이진 분류 문제에 사용할 수 있다. (로지스틱 회귀 분류기나 선형 SVM처럼)입력의 선형 조합을 계산해서 그 결과가 임곗값을 넘어서면 양성 클래스를 출력하고 그렇지 않으면 음성 클래스를 출력한다. 

예를 들어 TLU를 이용해 꽃잎의 길이와 너비를 기반으로 붓꽃의 품종을 분류할 수 있다. TLU를 훈련시킨다는 것은 최적의 $w_0, w_1, w_2$를 찾는다는 뜻이다. $w_0$의 경우 편향 특성 $x_0=1$을 추가해서 계산한다.

퍼셉트론은 층이 하나뿐인 TLU로 구성된다. 각 뉴런은 모든 입력에 연결되어 있다. 이 연결은 input neuron이라 부르는 특별한 통과 뉴런을 사용해 표현 한다. 이 뉴런은 무엇이 주입되든 입력을 그냥 출력으로 통과시킨다. 입력 2개와 출력 3개로 구성된 퍼셉트론의 경우 multioutput classifier 다충 출력 분류기라고 불린다.



그렇다면 퍼셉트론은 어떻게 훈련될까? 퍼셉트론의 훈련 알고리즘은 헤브의 규칙으로부터 영감을 많이 받았다. 생물학적 뉴런이 다른 뉴런을 활성화시킬 때 이 두 뉴런의 연결이 더 강해진다고 제안하였고 이 아이디어를 적용하고자 했다. 규칙과 다음과 같다.

###### 퍼셉트론 학습 규칙(가중치 업데이트)

​								$$w_{i,j}^{next step}=w_{i,j} + \eta(y_i-\hat{y_j}x_i)$$

1. $w_{i, j}$는 $i$번째 입력 뉴런과 $j$번째 출력 뉴런 사이를 결정하는 가중치이다.
2. $x_i$는 현재 훈련 샘플의 $i$번째 뉴런의 입력값이다.
3. $\hat{y_j}$는 현재 훈련 샘플의 $j$번째 출력 뉴런의 출력값이다.
4. $y_j$는 현재 훈련 샘플의 $j$번째 출력 뉴런의 타깃값이다.
5. $\eta​$은 학습률이다.

각 출력 뉴런의 결정 경계는 선형이므로 퍼셉트론도 복잡한 패턴을 학습하지 못한다. 하지만 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것을 보였다. 이를 perceptron convergence theorem이라고 한다. 

로지스틱 회귀 분류기와 달리 퍼셉트론은 클래스 확률을 제공하지 않으므로 고정된 임곗값을 기준으로 예측을 만든다. 그렇기 때문에 로지스틱 회귀 분류기를 선호하게 된다.

특히 퍼셉트론에서는 배타적 논리합과 같은 XOR 분류 문제를 풀 수 없다. 물론 이는 다른 선형 분류기도 마찬가지이다. 이러한 고수준의 문제를 연구하기 위해 신경망 연구를 떠날 수 밖에 없었다.

이를 극복하기 위해 여러개의 퍼셉트론을 쌓아올려 일부 제약을 줄일 수 있다는 사실이 밝혀졌다. 이런 인공 신경망을 Multi Layer Perceptron(MLP)라고 한다. MLP는 XOR을 풀 수 있는 것을 알 수 있다.



#### 10.1.4 다층 퍼셉트론과 역전파

MLP는 hidden layer(은닉층)이라 불리는 하나 이상의 TLU(threshold logic unit)층과 마지막 층인 output layer로 구성된다. 출력층을 제외하고 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결되어 있다. 인공 신경망의 은닉층이 2개 이상일 때 이를 DNN(deep neural network)라고 한다.

DNN을 훈련시킬 방법을 찾기 위해 노력한 결과 1986년 backpropagation 훈련 알고리즘을 개발했습니다. 요즘에는 이를 후진 모드 자동 미분을 사용하는 경사 하강법으로 기술한다.









### 10.2 텐서플로의 고수준 API로 다층 퍼셉트론 훈련하기

### 10.3 텐서플로의 저수준 API로 심층 신경망 훈련하기

### 10.4 신경망 하이퍼파라미터 튜닝하기

### 10.5 연습문제